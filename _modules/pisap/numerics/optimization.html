<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
  
    <title>PISAP &mdash; piSAP</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="../../../_static/css/bootstrap.min.css" media="screen" />

    
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '0.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="top" title="piSAP" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" />
  
   
       <script type="text/javascript" src="../../../_static/sidebar.js"></script>
   
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="../../../_static/js/bootstrap.min.js" type="text/javascript"></script>

  <script src="../../../_static/js/angular.min.js" type="text/javascript"></script>
  <script type="text/javascript">
    var app = angular.module("navApp", []);
    app.controller('navCtrl', ['$scope', '$window', function($scope, $window) {
      $scope.isActive = function (viewLocation) {
        var rel = $window.location.pathname.split("/")
        rel = rel[rel.length - 1]
        rel = rel.split(".")[0]
        var active = (viewLocation === rel);
        return active;};
    }]);
  </script>

  </head>
  <body role="document"><nav class="navbar navbar-default navbar-static-top">
  <div class="container-fluid">
    <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../index.html">
        <img alt="Logo" src="../../../_static/pisap.png">
        </a>
    </div>
    <div id="navbar" class="navbar-collapse collapse" ng-controller="navCtrl" ng-app="navApp">
      <ul class="nav navbar-nav">
        <li ng-class="{ active: isActive('index') }"><a href="../../../index.html">Home</a></li>
        <li ng-class="{ active: isActive('installation') }"><a href="../../../generated/installation.html">Installation</a></li>
        <li ng-class="{ active: isActive('documentation') }"><a href="../../../generated/documentation.html">Documentation</a></li>
        <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false"> <span class="caret"></span></a>
          <ul class="dropdown-menu">
	        <li><a href="../../../generated/pisap.html">Pisap</a></li>
<li><a href="../../../generated/pisap.base.html">Pisap.base</a></li>
<li><a href="../../../generated/pisap.sparsity.html">Pisap.sparsity</a></li>
<li><a href="../../../generated/pisap.stats.html">Pisap.stats</a></li>
<li><a href="../../../generated/pisap.numerics.html">Pisap.numerics</a></li>
<li><a href="../../../generated/pisap.extensions.html">Pisap.extensions</a></li>
<li><a href="../../../generated/pisap.plotting.html">Pisap.plotting</a></li>
<li><a href="../../../generated/pisap.base.loaders.html">Pisap.base.loaders</a></li>
          </ul>
        </li>
        <li><a href="http://cosmic.cosmostat.org/">COSMIC webPage</a></li>
      </ul>
    </div>
  </div>
</nav>


<div class="content-wrapper">
    <div class="sphinxsidebar">
      <div class="sphinxsidebarwrapper">

        <!-- info setup -->
          <p class="doc-version">
           This documentation is for pisap <strong>version 0.0.0</strong>
          </p>
        <p class="citing">
          If you use the software, please do not esitate to 
          <a &mdash; <a href="https://github.com/neurospin/pisap">
          Report a Bug</a>.
        </p>

      <!-- toc tree -->
      

      </div>
    </div>
  

  <div class="content">
      
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for pisap.numerics.optimization</h1><div class="highlight"><pre>
<span class="c1">##########################################################################</span>
<span class="c1"># XXX - Copyright (C) XXX, 2017</span>
<span class="c1"># Distributed under the terms of the CeCILL-B license, as published by</span>
<span class="c1"># the CEA-CNRS-INRIA. Refer to the LICENSE file or to</span>
<span class="c1"># http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html</span>
<span class="c1"># for details.</span>
<span class="c1">#</span>
<span class="c1">#:Author: Samuel Farrens &lt;samuel.farrens@gmail.com&gt;</span>
<span class="c1">#:Version: 1.1</span>
<span class="c1">#:Date: 05/01/2017</span>
<span class="c1">##########################################################################</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The Condat-Vu splitting method is a primal-dual proximal algorithm.</span>

<span class="sd">**References**</span>

<span class="sd">1) Condat, A Primal-Dual Splitting Method for Convex Optimization Involving</span>
<span class="sd">Lipschitzian, Proximable and Linear Composite Terms, 2013, Journal of</span>
<span class="sd">Optimization Theory and Applications, 158, 2, 460. (C2013)</span>
<span class="sd">2) Bauschke et al., Fixed-Point Algorithms for Inverse Problems in Science</span>
<span class="sd">and Engineering, 2011, Chapter 10. (B2010)</span>
<span class="sd">3) Raguet et al., Generalized Forward-Backward Splitting, 2012, , (R2012)</span>

<span class="sd">**Notes**</span>

<span class="sd">* x_old is used in place of x_{n}.</span>
<span class="sd">* x_new is used in place of x_{n+1}.</span>
<span class="sd">* x_prox is used in place of \~{x}_{n+1}.</span>
<span class="sd">* x_temp is used for intermediate operations.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># System import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span><span class="p">,</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">absolute_import</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">copy</span>


<div class="viewcode-block" id="FISTA"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.FISTA">[docs]</a><span class="k">class</span> <span class="nc">FISTA</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot; FISTA</span>

<span class="sd">    This class is inhereited by optimisation classes to speed up convergence</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lambda_init : float</span>
<span class="sd">        Initial value of the relaxation parameter</span>
<span class="sd">    active : bool</span>
<span class="sd">        Option to activate FISTA convergence speed-up (default is &#39;True&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambda_init</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">active</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_now</span> <span class="o">=</span> <span class="n">lambda_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_now</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_prev</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_speed_up</span> <span class="o">=</span> <span class="n">active</span>

<div class="viewcode-block" id="FISTA.speed_switch"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.FISTA.speed_switch">[docs]</a>    <span class="k">def</span> <span class="nf">speed_switch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">turn_on</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Speed swicth</span>

<span class="sd">        This method turns on or off the speed-up</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        turn_on : bool</span>
<span class="sd">            Option to turn on speed-up (default is &#39;True&#39;)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_speed_up</span> <span class="o">=</span> <span class="n">turn_on</span></div>

<div class="viewcode-block" id="FISTA.update_lambda"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.FISTA.update_lambda">[docs]</a>    <span class="k">def</span> <span class="nf">update_lambda</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Update lambda</span>

<span class="sd">        This method updates the value of lambda</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Implements steps 3 and 4 from algoritm 10.7 in B2010</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_prev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_now</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_now</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_prev</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_now</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t_prev</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_now</span></div>

<div class="viewcode-block" id="FISTA.speed_up"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.FISTA.speed_up">[docs]</a>    <span class="k">def</span> <span class="nf">speed_up</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Speed-up</span>

<span class="sd">        This method returns the update if the speed-up is active</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_speed_up</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_lambda</span><span class="p">()</span></div></div>


<div class="viewcode-block" id="ForwardBackward"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.ForwardBackward">[docs]</a><span class="k">class</span> <span class="nc">ForwardBackward</span><span class="p">(</span><span class="n">FISTA</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Forward-Backward optimisation</span>

<span class="sd">    This class implements standard forward-backward optimisation with an the</span>
<span class="sd">    option to use the FISTA speed-up</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : np.ndarray</span>
<span class="sd">        Initial guess for the primal variable</span>
<span class="sd">    grad : class</span>
<span class="sd">        Gradient operator class</span>
<span class="sd">    prox : class</span>
<span class="sd">        Proximity operator class</span>
<span class="sd">    cost : class</span>
<span class="sd">        Cost function class</span>
<span class="sd">    lambda_init : float</span>
<span class="sd">        Initial value of the relaxation parameter</span>
<span class="sd">    lambda_update :</span>
<span class="sd">        Relaxation parameter update method</span>
<span class="sd">    use_fista : bool</span>
<span class="sd">        Option to use FISTA (default is &#39;True&#39;)</span>
<span class="sd">    auto_iterate : bool</span>
<span class="sd">        Option to automatically begin iterations upon initialisation (default</span>
<span class="sd">        is &#39;True&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">prox</span><span class="p">,</span> <span class="n">cost</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">lambda_init</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">lambda_update</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">use_fista</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">auto_iterate</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">FISTA</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambda_init</span><span class="p">,</span> <span class="n">use_fista</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_old</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_old</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_old</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prox</span> <span class="o">=</span> <span class="n">prox</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost_func</span> <span class="o">=</span> <span class="n">cost</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_update</span> <span class="o">=</span> <span class="n">lambda_update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">converge</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">if</span> <span class="n">auto_iterate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iterate</span><span class="p">()</span>

<div class="viewcode-block" id="ForwardBackward.update"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.ForwardBackward.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Update</span>

<span class="sd">        This method updates the current reconstruction</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Implements algorithm 10.7 (or 10.5) from B2010</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Step 1 from alg.10.7.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">get_grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z_old</span><span class="p">)</span>
        <span class="n">y_old</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_old</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">inv_spec_rad</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad</span>

        <span class="c1"># Step 2 from alg.10.7.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prox</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">y_old</span><span class="p">)</span>

        <span class="c1"># Steps 3 and 4 from alg.10.7.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speed_up</span><span class="p">()</span>

        <span class="c1"># Step 5 from alg.10.7.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_old</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_now</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_new</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_old</span><span class="p">)</span>

        <span class="c1"># Test primal variable for convergence.</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z_old</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_new</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="mf">1e-6</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39; - converged!&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">converge</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="c1"># Update old values for next iteration.</span>
        <span class="n">np</span><span class="o">.</span><span class="n">copyto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_old</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_new</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">copyto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z_old</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_new</span><span class="p">)</span>

        <span class="c1"># Update parameter values for next iteration.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_update</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">None</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_now</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_now</span><span class="p">)</span>

        <span class="c1"># Test cost function for convergence.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_func</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">None</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">converge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_func</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z_new</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z_new</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The reconstruction is fucked!&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="ForwardBackward.iterate"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.ForwardBackward.iterate">[docs]</a>    <span class="k">def</span> <span class="nf">iterate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">150</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Iterate</span>

<span class="sd">        This method calls update until either convergence criteria is met or</span>
<span class="sd">        the maximum number of iterations is reached</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        max_iter : int, optional</span>
<span class="sd">            Maximum number of iterations (default is &#39;150&#39;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">converge</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s1">&#39; - Converged!&#39;</span><span class="p">)</span>
                <span class="k">break</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">x_final</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_new</span></div></div>


<div class="viewcode-block" id="GenForwardBackward"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.GenForwardBackward">[docs]</a><span class="k">class</span> <span class="nc">GenForwardBackward</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot; Generalized Forward-Backward optimisation</span>

<span class="sd">    This class implements algorithm 1 from R2012</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : np.ndarray</span>
<span class="sd">        Initial guess for the primal variable</span>
<span class="sd">    grad : class</span>
<span class="sd">        Gradient operator class</span>
<span class="sd">    prox_list : list</span>
<span class="sd">        List of proximity operator classes</span>
<span class="sd">    cost : class</span>
<span class="sd">        Cost function class</span>
<span class="sd">    lambda_init : float</span>
<span class="sd">        Initial value of the relaxation parameter</span>
<span class="sd">    lambda_update :</span>
<span class="sd">        Relaxation parameter update method</span>
<span class="sd">    weights : np.ndarray</span>
<span class="sd">        Proximity operator weights</span>
<span class="sd">    auto_iterate : bool</span>
<span class="sd">        Option to automatically begin iterations upon initialisation (default</span>
<span class="sd">        is &#39;True&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">prox_list</span><span class="p">,</span> <span class="n">cost</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">lambda_init</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">lambda_update</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">auto_iterate</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                 <span class="n">plot</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_old</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prox_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">prox_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost_func</span> <span class="o">=</span> <span class="n">cost</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_init</span> <span class="o">=</span> <span class="n">lambda_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_update</span> <span class="o">=</span> <span class="n">lambda_update</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">None</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">prox_list</span><span class="o">.</span><span class="n">size</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">prox_list</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

        <span class="c1"># Check weights.</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Proximity operator weights must sum to 1.0.&#39;</span>
                             <span class="s1">&#39;Current sum of weights = &#39;</span> <span class="o">+</span>
                             <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">x_old</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prox_list</span><span class="o">.</span><span class="n">size</span><span class="p">)])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">plot</span> <span class="o">=</span> <span class="n">plot</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">converge</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">if</span> <span class="n">auto_iterate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iterate</span><span class="p">()</span>

<div class="viewcode-block" id="GenForwardBackward.update"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.GenForwardBackward.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Update</span>

<span class="sd">        This method updates the current reconstruction</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Implements algorithm 1 from R2012</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Calculate gradient for current iteration.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">get_grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_old</span><span class="p">)</span>

        <span class="c1"># Update z values.</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prox_list</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
            <span class="n">z_temp</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_old</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">inv_spec_rad</span> <span class="o">*</span>
                      <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">z_prox</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prox_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">z_temp</span><span class="p">,</span>
                                          <span class="n">extra_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">inv_spec_rad</span> <span class="o">/</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_init</span> <span class="o">*</span> <span class="p">(</span><span class="n">z_prox</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_old</span><span class="p">)</span>

        <span class="c1"># Update current reconstruction.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">z_i</span> <span class="o">*</span> <span class="n">w_i</span> <span class="k">for</span> <span class="n">z_i</span><span class="p">,</span> <span class="n">w_i</span> <span class="ow">in</span>
                            <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Update old values for next iteration.</span>
        <span class="n">np</span><span class="o">.</span><span class="n">copyto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_old</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_new</span><span class="p">)</span>

        <span class="c1"># Update parameter values for next iteration.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_update</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">None</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lambda_now</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambda_now</span><span class="p">)</span>

        <span class="c1"># Test cost function for convergence.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_func</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">None</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">converge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_func</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_new</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_new</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The deconvolution is fucked!&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="GenForwardBackward.iterate"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.GenForwardBackward.iterate">[docs]</a>    <span class="k">def</span> <span class="nf">iterate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">150</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Iterate</span>

<span class="sd">        This method calls update until either convergence criteria is met or</span>
<span class="sd">        the maximum number of iterations is reached</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        max_iter : int, optional</span>
<span class="sd">            Maximum number of iterations (default is &#39;150&#39;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">converge</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s1">&#39; - Converged!&#39;</span><span class="p">)</span>
                <span class="k">break</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">x_final</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_new</span></div></div>
        <span class="c1"># self.cost_func.plot_cost()</span>


<div class="viewcode-block" id="Condat"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.Condat">[docs]</a><span class="k">class</span> <span class="nc">Condat</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot; Condat optimisation</span>

<span class="sd">    This class implements algorithm 10.7 from C2013</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : np.ndarray</span>
<span class="sd">        Initial guess for the primal variable</span>
<span class="sd">    y : np.ndarray</span>
<span class="sd">        Initial guess for the dual variable</span>
<span class="sd">    grad : class</span>
<span class="sd">        Gradient operator class</span>
<span class="sd">    prox : class</span>
<span class="sd">        Proximity primal operator class</span>
<span class="sd">    prox_dual : class</span>
<span class="sd">        Proximity dual operator class</span>
<span class="sd">    linear : class</span>
<span class="sd">        Linear operator class</span>
<span class="sd">    cost : class</span>
<span class="sd">        Cost function class</span>
<span class="sd">    rho : float</span>
<span class="sd">        Relaxation parameter</span>
<span class="sd">    sigma : float</span>
<span class="sd">        Proximal dual parameter</span>
<span class="sd">    tau : float</span>
<span class="sd">        Proximal primal paramater</span>
<span class="sd">    rho_update :</span>
<span class="sd">        Relaxation parameter update method</span>
<span class="sd">    sigma_update :</span>
<span class="sd">        Proximal dual parameter update method</span>
<span class="sd">    tau_update :</span>
<span class="sd">        Proximal primal parameter update method</span>
<span class="sd">    extra_factor_update :</span>
<span class="sd">        Extra factor passed to the dual proximity operator update</span>
<span class="sd">    auto_iterate : bool</span>
<span class="sd">        Option to automatically begin iterations upon initialisation (default</span>
<span class="sd">        is &#39;True&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">prox</span><span class="p">,</span> <span class="n">prox_dual</span><span class="p">,</span> <span class="n">linear</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span>
                 <span class="n">rho</span><span class="p">,</span>  <span class="n">sigma</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rho_update</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">sigma_update</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">tau_update</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">extra_factor_update</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">auto_iterate</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_old</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_old</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prox</span> <span class="o">=</span> <span class="n">prox</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prox_dual</span> <span class="o">=</span> <span class="n">prox_dual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost_func</span> <span class="o">=</span> <span class="n">cost</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">rho</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rho_update</span> <span class="o">=</span> <span class="n">rho_update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma_update</span> <span class="o">=</span> <span class="n">sigma_update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau_update</span> <span class="o">=</span> <span class="n">tau_update</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">converge</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extra_factor</span> <span class="o">=</span> <span class="mf">1.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">extra_factor_update</span> <span class="o">=</span> <span class="n">extra_factor_update</span>
        <span class="k">if</span> <span class="n">auto_iterate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">iterate</span><span class="p">()</span>

<div class="viewcode-block" id="Condat.update_param"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.Condat.update_param">[docs]</a>    <span class="k">def</span> <span class="nf">update_param</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Update parameters</span>

<span class="sd">        This method updates the values of rho, sigma and tau with the methods</span>
<span class="sd">        provided</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Update relaxation parameter.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rho_update</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rho</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rho_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rho</span><span class="p">)</span>

        <span class="c1"># Update proximal dual parameter.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_update</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>

        <span class="c1"># Update proximal primal parameter.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau_update</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">)</span>

        <span class="c1"># Update the dual proximity extra factor</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_factor_update</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extra_factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_factor_update</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">)</span></div>

<div class="viewcode-block" id="Condat.update"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.Condat.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Update</span>

<span class="sd">        This method updates the current reconstruction</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Implements equation 9 (algorithm 3.1) from C2013</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Update parameter values for next iteration.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">get_grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_old</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_param</span><span class="p">()</span>

        <span class="c1"># Step 1 from eq.9.</span>
        <span class="n">x_temp</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_old</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">grad</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">*</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">adj_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_old</span><span class="p">))</span>
        <span class="n">x_prox</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prox</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">x_temp</span><span class="p">)</span>

        <span class="c1"># Step 2 from eq.9.</span>
        <span class="n">y_temp</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_old</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x_prox</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_old</span><span class="p">))</span>
        <span class="n">y_prox</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_temp</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">*</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">prox_dual</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">y_temp</span><span class="p">,</span> <span class="n">extra_factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">extra_factor</span><span class="p">))</span>

        <span class="c1"># Step 3 from eq.9.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rho</span> <span class="o">*</span> <span class="n">x_prox</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_old</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rho</span> <span class="o">*</span> <span class="n">y_prox</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_old</span>

        <span class="c1"># Update old values for next iteration.</span>
        <span class="n">np</span><span class="o">.</span><span class="n">copyto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_old</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_new</span><span class="p">)</span>
        <span class="n">y_old</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_new</span><span class="p">)</span>

        <span class="c1"># Test cost function for convergence.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">converge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_func</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_new</span><span class="p">)</span></div>

<div class="viewcode-block" id="Condat.iterate"><a class="viewcode-back" href="../../../generated/pisap.numerics.optimization.html#pisap.numerics.optimization.Condat.iterate">[docs]</a>    <span class="k">def</span> <span class="nf">iterate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">150</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Iterate</span>

<span class="sd">        This method calls update until either convergence criteria is met or</span>
<span class="sd">        the maximum number of iterations is reached</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        max_iter : int, optional</span>
<span class="sd">            Maximum number of iterations (default is &#39;150&#39;)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">converge</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot; - Converged!&quot;</span><span class="p">)</span>
                <span class="k">break</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">x_final</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_new</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_final</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_new</span></div></div>
        <span class="c1"># self.cost_func.plot_cost()</span>
</pre></div>

          </div>
        </div>
      </div>
    <div class="clearer">
    </div>
  </div>
  
</div>

<div class="footer">
    &copy; 2017, piSAP developers &lt;XXX&gt;.
</div>
  </body>
</html>